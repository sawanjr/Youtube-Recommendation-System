{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset Class for Collaborative Filtering\n",
    "class CollabDataset(Dataset):\n",
    "    def __init__(self, df, user_col=0, item_col=1, rating_col=2):\n",
    "        self.df = df\n",
    "        self.user_tensor = torch.tensor(self.df.iloc[:, user_col], dtype=torch.long, device=device)\n",
    "        self.item_tensor = torch.tensor(self.df.iloc[:, item_col], dtype=torch.long, device=device)\n",
    "        self.target_tensor = torch.tensor(self.df.iloc[:, rating_col], dtype=torch.float32, device=device)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.user_tensor[index], self.item_tensor[index], self.target_tensor[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.user_tensor.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load data\n",
    "def load_data(data_path, id_val=1):\n",
    "    # Load train and validation data\n",
    "    train_df = pd.read_csv(f'{data_path}u{id_val}.base', sep='\\t', header=None)\n",
    "    train_df.columns = ['user_id', 'item_id', 'rating', 'ts']\n",
    "    train_df['user_id'] = train_df['user_id'] - 1\n",
    "    train_df['item_id'] = train_df['item_id'] - 1\n",
    "\n",
    "    valid_df = pd.read_csv(f'{data_path}u{id_val}.test', sep='\\t', header=None)\n",
    "    valid_df.columns = ['user_id', 'item_id', 'rating', 'ts']\n",
    "    valid_df['user_id'] = valid_df['user_id'] - 1\n",
    "    valid_df['item_id'] = valid_df['item_id'] - 1\n",
    "\n",
    "    return train_df, valid_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get DataLoaders\n",
    "def get_data_loaders(train_df, valid_df, batch_size=2000):\n",
    "    train_dataset = CollabDataset(train_df)\n",
    "    valid_dataset = CollabDataset(valid_df)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Neural Network for the Recommendation System\n",
    "class ConcatNet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ConcatNet, self).__init__()\n",
    "        self.config = config\n",
    "        self.num_users = config['num_users']\n",
    "        self.num_items = config['num_items']\n",
    "        self.emb_size = config['emb_size']\n",
    "        self.emb_dropout = config['emb_dropout']\n",
    "        self.fc_layer_sizes = config['fc_layer_sizes']\n",
    "        self.dropout = config['dropout']\n",
    "        self.out_range = config['out_range']\n",
    "\n",
    "        self.emb_user = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.emb_size),\n",
    "            nn.Dropout(p=self.emb_dropout))\n",
    "        self.emb_item = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.emb_size),\n",
    "            nn.Dropout(p=self.emb_dropout))\n",
    "\n",
    "        fc_layers_list = []\n",
    "        for ni, nf, p in zip(self.fc_layer_sizes[:-1], self.fc_layer_sizes[1:], self.dropout):\n",
    "            fc_layers_list.append(nn.Linear(ni, nf))\n",
    "            fc_layers_list.append(nn.ReLU(inplace=True))\n",
    "            fc_layers_list.append(nn.BatchNorm1d(nf))\n",
    "            fc_layers_list.append(nn.Dropout(p=p))\n",
    "        self.fc_layers = nn.Sequential(*fc_layers_list)\n",
    "\n",
    "        self.head = torch.nn.Linear(in_features=self.fc_layer_sizes[-1], out_features=1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.emb_user[0].weight)\n",
    "        nn.init.xavier_uniform_(self.emb_item[0].weight)\n",
    "\n",
    "    def forward(self, user_idx, item_idx):\n",
    "        user_emb = self.emb_user(user_idx)\n",
    "        item_emb = self.emb_item(item_idx)\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)\n",
    "        x = self.fc_layers(x)\n",
    "        x = torch.sigmoid(self.head(x))\n",
    "        x = x * (self.out_range[1] - self.out_range[0]) + self.out_range[0]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for the model\n",
    "config = {\n",
    "    'num_users': 943,\n",
    "    'num_items': 1682,\n",
    "    'emb_size': 50,\n",
    "    'emb_dropout': 0.05,\n",
    "    'fc_layer_sizes': [100, 512, 256],\n",
    "    'dropout': [0.7, 0.35],\n",
    "    'out_range': [0.8, 5.2]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(data_path, num_epochs=5, batch_size=2000, learning_rate=1e-2, weight_decay=5e-1):\n",
    "    train_df, valid_df = load_data(data_path)\n",
    "    train_loader, valid_loader = get_data_loaders(train_df, valid_df, batch_size)\n",
    "\n",
    "    model = ConcatNet(config).to(device)\n",
    "    criterion = torch.nn.MSELoss(reduction='sum')  # Using sum to compute total loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    valid_rmse = []\n",
    "    valid_mae = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for u, i, r in train_loader:\n",
    "            u, i, r = u.to(device), i.to(device), r.to(device)\n",
    "            r_pred = model(u, i)\n",
    "            loss = criterion(r_pred, r[:, None])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        total_abs_error = 0\n",
    "        total_squared_error = 0\n",
    "        num_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for u, i, r in valid_loader:\n",
    "                u, i, r = u.to(device), i.to(device), r.to(device)\n",
    "                r_pred = model(u, i)\n",
    "                valid_loss += criterion(r_pred, r[:, None]).item()\n",
    "                \n",
    "                # Accumulate MAE and RMSE\n",
    "                abs_error = torch.abs(r_pred - r[:, None]).sum().item()\n",
    "                squared_error = torch.sum((r_pred - r[:, None]) ** 2).item()\n",
    "                \n",
    "                total_abs_error += abs_error\n",
    "                total_squared_error += squared_error\n",
    "                num_samples += len(r)\n",
    "\n",
    "        valid_loss /= len(valid_loader.dataset)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        # Calculate MAE and RMSE\n",
    "        mae = total_abs_error / num_samples\n",
    "        rmse = np.sqrt(total_squared_error / num_samples)\n",
    "\n",
    "        valid_rmse.append(rmse)\n",
    "        valid_mae.append(mae)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss}, Valid Loss = {valid_loss}, RMSE = {rmse}, MAE = {mae}\")\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "\n",
    "    # Load the best model and save it\n",
    "    model.load_state_dict(best_model)\n",
    "    torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    # Plot the losses and metrics\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Valid Loss')\n",
    "    plt.plot(valid_rmse, label='RMSE')\n",
    "    plt.plot(valid_mae, label='MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "# !pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "# !pip install --upgrade jupyter ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Call the train_model function\n",
    "data_path = \"E:\\Projects\\youtube-Recomendation system/notebooks/ml-100k/\"  # Replace with the correct path to your data\n",
    "train_model(data_path, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ASUS'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
